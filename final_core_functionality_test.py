#!/usr/bin/env python3
"""
CORE FUNCTIONALITY TEST - Authentication Olmadan
Direct service calls ile tam sistem testini yapacaÄŸÄ±z
"""
import asyncio
import json
from io import BytesIO
import os

async def test_core_functionality():
    """Core functionality comprehensive test"""
    
    print("ğŸ¯ CORE FUNCTIONALITY TEST")
    print("=" * 28)
    print("Authentication bypassed - Direct service testing")
    
    try:
        # Test 1: Elasticsearch Health & Storage
        await test_elasticsearch_comprehensive()
        
        # Test 2: Document Processing Simulation  
        await test_document_processing_pipeline()
        
        # Test 3: Advanced Search Tests
        await test_advanced_search_scenarios()
        
        # Test 4: AI Integration Test
        await test_ai_integration_direct()
        
        # Test 5: Performance Benchmarks
        await test_performance_benchmarks()
        
        print("\nğŸ† CORE FUNCTIONALITY TEST COMPLETED!")
        print("=" * 37)
        
    except Exception as e:
        print(f"âŒ Core test error: {e}")
        import traceback
        traceback.print_exc()

async def test_elasticsearch_comprehensive():
    """Comprehensive Elasticsearch testing"""
    print("\n1ï¸âƒ£ ELASTICSEARCH COMPREHENSIVE TEST")
    
    try:
        from services.elasticsearch_service import ElasticsearchService
        from services.embedding_service import EmbeddingService
        
        # Elasticsearch health
        es_service = ElasticsearchService()
        health = await es_service.health_check()
        
        print(f"âœ… Cluster Status: {health['cluster_status']}")
        print(f"âœ… Document Count: {health['document_count']}")
        print(f"âœ… Vector Dimensions: {health['vector_dimensions']}")
        
        # Index details
        try:
            index_info = await es_service.get_index_info()
            print(f"âœ… Index Settings: {index_info}")
        except:
            print("âš ï¸ Index info not available")
        
        # Embedding service test
        embedding_service = EmbeddingService()
        total_embeddings = await embedding_service.get_embeddings_count()
        print(f"âœ… Total Embeddings: {total_embeddings}")
        
        # Test embedding generation
        test_text = "Sosyal gÃ¼venlik mevzuatÄ± test metni"
        embedding = await embedding_service.generate_embedding(test_text)
        print(f"âœ… Embedding Generation: {len(embedding)}D vector")
        
    except Exception as e:
        print(f"âŒ Elasticsearch test error: {e}")

async def test_document_processing_pipeline():
    """Document processing pipeline simulation"""
    print("\n2ï¸âƒ£ DOCUMENT PROCESSING PIPELINE TEST")
    
    try:
        from tasks.document_processor import DocumentProcessor
        from services.embedding_service import EmbeddingService
        
        # Create test document content
        test_content = """
        SOSYAL GÃœVENLÄ°K KANUNU TEST DOKÃœMANI
        
        Madde 1: SigortalÄ±lÄ±k ÅartlarÄ±
        Bu kanun kapsamÄ±nda sigortalÄ± sayÄ±lanlar:
        a) 18 yaÅŸÄ±nÄ± doldurmuÅŸ TÃ¼rkiye Cumhuriyeti vatandaÅŸlarÄ±
        b) AylÄ±k geliri asgari Ã¼cretin yarÄ±sÄ±nÄ± geÃ§enler
        c) Ã‡alÄ±ÅŸma izni olan yabancÄ± uyruklu kiÅŸiler
        
        Madde 2: Prim Ã–deme YÃ¼kÃ¼mlÃ¼lÃ¼ÄŸÃ¼
        SigortalÄ±nÄ±n ve iÅŸverenin prim Ã¶deme yÃ¼kÃ¼mlÃ¼lÃ¼ÄŸÃ¼ vardÄ±r.
        Primler her ayÄ±n 23'Ã¼ne kadar SGK'ya yatÄ±rÄ±lÄ±r.
        Geciken primler iÃ§in yasal faiz uygulanÄ±r.
        
        Madde 3: Emeklilik ÅartlarÄ±
        Erkekler: 65 yaÅŸ ve 7200 gÃ¼n prim
        KadÄ±nlar: 63 yaÅŸ ve 6300 gÃ¼n prim
        Erken emeklilik: 25 yÄ±l sigortalÄ±lÄ±k sÃ¼resi
        
        Madde 4: Ä°ÅŸ KazasÄ± TazminatlarÄ±
        GeÃ§ici iÅŸ gÃ¶remezlik Ã¶deneÄŸi gÃ¼nlÃ¼k kazancÄ±n %66.6'sÄ±
        SÃ¼rekli iÅŸ gÃ¶remezlik durumunda aylÄ±k baÄŸlanÄ±r
        Ã–lÃ¼m halinde dul ve yetim aylÄ±klarÄ± verilir
        """
        
        # Text chunking simulation
        from langchain.text_splitter import RecursiveCharacterTextSplitter
        
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=100,
            length_function=len,
        )
        
        chunks = text_splitter.split_text(test_content)
        print(f"âœ… Text Chunking: {len(chunks)} chunks created")
        
        # Embedding generation for each chunk
        embedding_service = EmbeddingService()
        
        test_document_id = "test-comprehensive-doc"
        chunks_data = []
        
        for i, chunk in enumerate(chunks):
            if chunk.strip():  # Skip empty chunks
                embedding = await embedding_service.generate_embedding(chunk)
                
                chunk_data = {
                    "content": chunk.strip(),
                    "embedding": embedding,
                    "chunk_index": i,
                    "page_number": 1,
                    "line_start": i * 10,
                    "line_end": (i + 1) * 10,
                    "source_institution": "SGK",
                    "source_document": "sosyal_guvenlik_kanunu_test.pdf",
                    "metadata": {
                        "test_document": True,
                        "comprehensive_test": True
                    }
                }
                chunks_data.append(chunk_data)
        
        # Store embeddings
        embedding_ids = await embedding_service.store_embeddings(
            document_id=test_document_id,
            chunks=chunks_data
        )
        
        print(f"âœ… Embeddings Stored: {len(embedding_ids)} vectors")
        print(f"âœ… Document ID: {test_document_id}")
        
        # Verify storage
        doc_count = await embedding_service.get_embeddings_count(test_document_id)
        print(f"âœ… Stored Verification: {doc_count} embeddings for document")
        
        return test_document_id
        
    except Exception as e:
        print(f"âŒ Document processing error: {e}")
        import traceback
        traceback.print_exc()
        return None

async def test_advanced_search_scenarios():
    """Advanced search scenarios testing"""
    print("\n3ï¸âƒ£ ADVANCED SEARCH SCENARIOS")
    
    try:
        from services.embedding_service import EmbeddingService
        
        embedding_service = EmbeddingService()
        
        # Complex search scenarios
        search_scenarios = [
            {
                "query": "sigortalÄ±lÄ±k ÅŸartlarÄ± yaÅŸ sÄ±nÄ±rlarÄ±",
                "expected_topics": ["sigortalÄ±lÄ±k", "yaÅŸ"],
                "description": "Age limit eligibility search"
            },
            {
                "query": "prim Ã¶deme yÃ¼kÃ¼mlÃ¼lÃ¼ÄŸÃ¼ gecikme cezasÄ±",
                "expected_topics": ["prim", "gecikme", "faiz"],
                "description": "Premium payment delay penalties"
            },
            {
                "query": "emeklilik koÅŸullarÄ± erkek kadÄ±n farkÄ±",
                "expected_topics": ["emeklilik", "erkek", "kadÄ±n"],
                "description": "Retirement conditions gender differences"
            },
            {
                "query": "iÅŸ kazasÄ± tazminat miktarÄ± hesaplama",
                "expected_topics": ["iÅŸ kazasÄ±", "tazminat", "hesaplama"],
                "description": "Work accident compensation calculation"
            },
            {
                "query": "sosyal gÃ¼venlik kapsamÄ± yabancÄ± iÅŸÃ§i",
                "expected_topics": ["yabancÄ±", "Ã§alÄ±ÅŸma izni"],
                "description": "Foreign worker social security coverage"
            }
        ]
        
        for scenario in search_scenarios:
            print(f"\nğŸ” Test: {scenario['description']}")
            print(f"   Query: '{scenario['query']}'")
            
            results = await embedding_service.similarity_search(
                query_text=scenario['query'],
                k=3,
                similarity_threshold=0.1
            )
            
            print(f"âœ… Results: {len(results)} matches found")
            
            if results:
                best_match = results[0]
                print(f"   Best Match Similarity: {best_match['similarity']:.3f}")
                print(f"   Content Preview: {best_match['content'][:80]}...")
                
                # Check if expected topics are in results
                content_lower = best_match['content'].lower()
                found_topics = [topic for topic in scenario['expected_topics'] 
                              if topic.lower() in content_lower]
                
                if found_topics:
                    print(f"   âœ… Found Expected Topics: {found_topics}")
                else:
                    print(f"   âš ï¸ Expected topics not found in top result")
        
    except Exception as e:
        print(f"âŒ Advanced search error: {e}")

async def test_ai_integration_direct():
    """Direct AI integration testing"""
    print("\n4ï¸âƒ£ AI INTEGRATION TEST (Direct)")
    
    try:
        from services.groq_service import GroqService
        from services.embedding_service import EmbeddingService
        
        # Get some sample search results
        embedding_service = EmbeddingService()
        
        search_results = await embedding_service.similarity_search(
            query_text="sigortalÄ±lÄ±k ÅŸartlarÄ± nelerdir",
            k=3,
            similarity_threshold=0.2
        )
        
        if search_results:
            print(f"âœ… Search Context: {len(search_results)} relevant documents")
            
            # Test AI response generation
            groq_service = GroqService()
            
            # Build context from search results
            context_parts = []
            for result in search_results:
                context_parts.append(f"Kaynak: {result['source_document']}")
                context_parts.append(f"Ä°Ã§erik: {result['content']}")
                context_parts.append("---")
            
            context = "\n".join(context_parts)
            
            query = "Sosyal gÃ¼venlik kapsamÄ±nda sigortalÄ±lÄ±k ÅŸartlarÄ± nelerdir?"
            
            # Generate AI response
            ai_response = await groq_service.generate_response(
                query=query,
                context=context,
                max_tokens=500
            )
            
            print(f"âœ… AI Response Generated: {len(ai_response)} characters")
            print(f"ğŸ“„ Response Preview: {ai_response[:150]}...")
            
            # Test response quality
            if any(keyword in ai_response.lower() for keyword in ['sigorta', 'yaÅŸ', 'ÅŸart']):
                print("âœ… Response Quality: Contains relevant keywords")
            else:
                print("âš ï¸ Response Quality: May lack specific keywords")
                
        else:
            print("âš ï¸ No search results to test AI integration")
            
    except Exception as e:
        print(f"âŒ AI integration error: {e}")

async def test_performance_benchmarks():
    """Performance benchmarks"""
    print("\n5ï¸âƒ£ PERFORMANCE BENCHMARKS")
    
    try:
        from services.embedding_service import EmbeddingService
        import time
        
        embedding_service = EmbeddingService()
        
        # Embedding generation speed
        test_text = "Bu bir performans testi metnidir"
        
        start_time = time.time()
        embedding = await embedding_service.generate_embedding(test_text)
        embedding_time = time.time() - start_time
        
        print(f"âœ… Embedding Generation: {embedding_time:.3f}s for 2048D vector")
        
        # Search speed test
        search_queries = [
            "sigortalÄ±lÄ±k ÅŸartlarÄ±",
            "prim Ã¶deme",
            "emeklilik yaÅŸÄ±",
            "iÅŸ kazasÄ±",
            "sosyal gÃ¼venlik"
        ]
        
        search_times = []
        for query in search_queries:
            start_time = time.time()
            results = await embedding_service.similarity_search(
                query_text=query,
                k=5,
                similarity_threshold=0.1
            )
            search_time = time.time() - start_time
            search_times.append(search_time)
            
        avg_search_time = sum(search_times) / len(search_times)
        
        print(f"âœ… Average Search Time: {avg_search_time:.3f}s per query")
        print(f"âœ… Fastest Search: {min(search_times):.3f}s")
        print(f"âœ… Slowest Search: {max(search_times):.3f}s")
        
        # Memory efficiency check
        total_embeddings = await embedding_service.get_embeddings_count()
        estimated_memory = total_embeddings * 2048 * 4 / (1024*1024)  # MB
        
        print(f"âœ… Memory Efficiency: ~{estimated_memory:.1f}MB for {total_embeddings} vectors")
        
    except Exception as e:
        print(f"âŒ Performance test error: {e}")

if __name__ == "__main__":
    asyncio.run(test_core_functionality())